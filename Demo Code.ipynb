{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data to panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read relevant columns to panda dataframe\n",
    "\n",
    "# please use your own path for this\n",
    "path = \"./\"\n",
    "admission = pd.read_csv(path + 'ADMISSIONS.csv', usecols=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'DISCHTIME', \n",
    "                                                   'DEATHTIME', 'ADMISSION_TYPE', 'DISCHARGE_LOCATION', 'DIAGNOSIS'])\n",
    "# convert admission time and discharge time death time to correct format\n",
    "admission.ADMITTIME = pd.to_datetime(admission.ADMITTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "admission.DISCHTIME = pd.to_datetime(admission.DISCHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "admission.DEATHTIME = pd.to_datetime(admission.DEATHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort before group by\n",
    "admission = admission.sort_values(['SUBJECT_ID','ADMITTIME'])\n",
    "admission = admission.reset_index(drop = True)\n",
    "\n",
    "# add the next admission date and type for each subject \n",
    "admission['NEXT_ADMITTIME'] = admission.groupby('SUBJECT_ID').ADMITTIME.shift(-1)\n",
    "admission['NEXT_ADMISSION_TYPE'] = admission.groupby('SUBJECT_ID').ADMISSION_TYPE.shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "admission = admission.sort_values(['SUBJECT_ID','ADMITTIME'])\n",
    "# back fill\n",
    "admission[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']] = admission.groupby(['SUBJECT_ID'])[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']].fillna(method = 'bfill')\n",
    "# compute days elapsed until next readmission\n",
    "admission['DAYS_NEXT_ADMIT']=  (admission.NEXT_ADMITTIME - admission.DISCHTIME).dt.total_seconds()/(24*60*60)\n",
    "# number of records that were readmitted in less than or equal to 30 days: 3390\n",
    "records = admission[admission.DAYS_NEXT_ADMIT <= 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miaor/CSE258/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3018: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# read the notes table \n",
    "notes = pd.read_csv(path + \"NOTEEVENTS.csv\")\n",
    "discharge_sum = notes.loc[notes.CATEGORY == 'Discharge summary']\n",
    "notes_dis_sum_last = (discharge_sum.groupby(['SUBJECT_ID','HADM_ID']).nth(-1)).reset_index()\n",
    "\n",
    "# perform a left join of the two datatable\n",
    "dt_table = pd.merge(admission,notes_dis_sum_last[['SUBJECT_ID','HADM_ID','TEXT']], on = ['SUBJECT_ID','HADM_ID'],how = 'left')\n",
    "# filter out new born records because a lot of them don't have discharge summary\n",
    "dt_table = dt_table[dt_table.ADMISSION_TYPE != 'NEWBORN']\n",
    "# filter out records that do not have discharge summary\n",
    "\n",
    "# add a column for label\n",
    "dt_table['LABEL'] = (dt_table.DAYS_NEXT_ADMIT <= 30).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a smaller test set \n",
    "# shuffle the dataset first:\n",
    "dt_table_shuffled = dt_table.sample(n=len(dt_table), random_state=42)\n",
    "dt_table_shuffled = dt_table_shuffled.reset_index(drop=True)\n",
    "dt_train = dt_table_shuffled.sample(frac=0.80, random_state=42)\n",
    "dt_val_test = dt_table_shuffled.drop(dt_train.index)\n",
    "dt_val = dt_val_test.sample(frac=0.50, random_state=42)\n",
    "dt_test = dt_val_test.drop(dt_val.index)\n",
    "\n",
    "# sub-sampling negative data:\n",
    "posRow = dt_train.LABEL==1\n",
    "dt_train_pos = dt_train.loc[posRow]\n",
    "dt_train_neg = dt_train.loc[~posRow]\n",
    "dt_train_sub = pd.concat([dt_train_pos, dt_train_neg.sample(n=len(dt_train_pos), random_state=42)], axis=0)\n",
    "# re-shuffle sub-sampled training dataset:\n",
    "dt_train_sub = dt_train_sub.sample(n=len(dt_train_sub), random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5111, 14)\n",
      "(5111, 14)\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing:\n",
    "# Fill missing notes with space and remove CRLF\n",
    "# Tokenize free-text\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "def my_tokenizer(text):\n",
    "    punc_list = string.punctuation + '0123456789'\n",
    "    tranTable = str.maketrans(dict.fromkeys(punc_list, \" \"))\n",
    "    text = text.lower().translate(tranTable)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def preprocess_text(df):\n",
    "    # This function preprocesses the text by filling not a number and replacing new lines ('\\n') and carriage returns ('\\r')\n",
    "    df.TEXT = df.TEXT.fillna(' ')\n",
    "    return df\n",
    "\n",
    "print(dt_test.shape)\n",
    "dt_train_sub = preprocess_text(dt_train_sub)\n",
    "dt_val = preprocess_text(dt_val)\n",
    "dt_test = preprocess_text(dt_test)\n",
    "print (dt_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 14)\n"
     ]
    }
   ],
   "source": [
    "dt_demo = dt_test[:20]\n",
    "print (dt_demo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert tokens into word vectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "rawCountVec = CountVectorizer(max_features=3000, tokenizer=my_tokenizer, ngram_range=(1, 2), stop_words=stop_words)\n",
    "tfidfVec = TfidfVectorizer(ngram_range=(1, 3), tokenizer=my_tokenizer, min_df=3, max_df=0.9, \n",
    "                           strip_accents='unicode', use_idf=True, smooth_idf=True, sublinear_tf=True, stop_words=stop_words)\n",
    "rawX_train = rawCountVec.fit_transform(dt_train_sub.TEXT.values)\n",
    "rawX_val = rawCountVec.transform(dt_val.TEXT.values)\n",
    "rawX_test = rawCountVec.transform(dt_test.TEXT.values)\n",
    "tfidftX_train = tfidfVec.fit_transform(dt_train_sub.TEXT.values)\n",
    "tfidfX_val = tfidfVec.transform(dt_val.TEXT.values)\n",
    "tfidfX_test = tfidfVec.transform(dt_test.TEXT.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5112, 380249)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfX_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models \n",
    "# Model 1: SVM with BoW\n",
    "with open('svm_model.pickle', 'rb') as handle:\n",
    "    svm_bow = pickle.load(handle)\n",
    "# Model 2: SVM with TFIDF\n",
    "with open('svm_model_tfidf.pickle', 'rb') as handle:\n",
    "    svm_tfidf= pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SVM using BOW Feature 0.45\n",
      "Accuracy for SVM using TFIDF Feature 0.55\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# sample demo \n",
    "# get label \n",
    "y_demo = dt_demo.LABEL.values\n",
    "# get feature \n",
    "x_demo_bow = rawX_test[:20]\n",
    "x_demo_tfidf = tfidfX_test[:20]\n",
    "# prediction \n",
    "y_pred = svm_bow.predict(x_demo_bow)\n",
    "svm_bow_accuracy = accuracy_score(y_demo, y_pred)\n",
    "print (\"Accuracy for SVM using BOW Feature\", svm_bow_accuracy)\n",
    "y_pred = svm_tfidf.predict(x_demo_tfidf)\n",
    "svm_tfidf_accuracy = accuracy_score(y_demo, y_pred)\n",
    "print (\"Accuracy for SVM using TFIDF Feature\", svm_tfidf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse258",
   "language": "python",
   "name": "cse258"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
